{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class HMM(torch.nn.Module):\n",
    "  \"\"\"\n",
    "  Hidden Markov Model with discrete observations.\n",
    "  \"\"\"\n",
    "  def __init__(self, M, N):\n",
    "    super(HMM, self).__init__()\n",
    "    self.M = M # number of possible observations\n",
    "    self.N = N # number of states\n",
    "\n",
    "    # A\n",
    "    self.transition_model = TransitionModel(self.N)\n",
    "\n",
    "    # b(x_t)\n",
    "    self.emission_model = EmissionModel(self.N,self.M)\n",
    "\n",
    "    # pi\n",
    "    self.unnormalized_state_priors = torch.nn.Parameter(torch.randn(self.N))\n",
    "\n",
    "    # use the GPU\n",
    "    self.is_cuda = torch.cuda.is_available()\n",
    "    if self.is_cuda: self.cuda()\n",
    "\n",
    "class TransitionModel(torch.nn.Module):\n",
    "  def __init__(self, N):\n",
    "    super(TransitionModel, self).__init__()\n",
    "    self.N = N\n",
    "    self.unnormalized_transition_matrix = torch.nn.Parameter(torch.randn(N,N))\n",
    "\n",
    "class EmissionModel(torch.nn.Module):\n",
    "  def __init__(self, N, M):\n",
    "    super(EmissionModel, self).__init__()\n",
    "    self.N = N\n",
    "    self.M = M\n",
    "    self.unnormalized_emission_matrix = torch.nn.Parameter(torch.randn(N,M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "#alphabet = string.ascii_lowercase\n",
    "alphabet = 'abc'\n",
    "\n",
    "def encode(s):\n",
    "  \"\"\"\n",
    "  Convert a string into a list of integers\n",
    "  \"\"\"\n",
    "  x = [alphabet.index(ss) for ss in s]\n",
    "  return x\n",
    "\n",
    "def decode(x):\n",
    "  \"\"\"\n",
    "  Convert list of ints to string\n",
    "  \"\"\"\n",
    "  s = \"\".join([alphabet[xx] for xx in x])\n",
    "  return s\n",
    "\n",
    "# Initialize the model\n",
    "model = HMM(M=len(alphabet), N=2) \n",
    "\n",
    "# Hard-wiring the parameters (the input is in exp(x) >> need to log p)!\n",
    "# Let state 0 = consonant, state 1 = vowel\n",
    "model.unnormalized_state_priors.data[0] = np.log(0.6)   \n",
    "model.unnormalized_state_priors.data[1] = np.log(0.4)\n",
    "print(\"State priors:\", torch.nn.functional.softmax(model.unnormalized_state_priors, dim=0))\n",
    "\n",
    "model.emission_model.unnormalized_emission_matrix.data[0, 0] = np.log(0.1)\n",
    "model.emission_model.unnormalized_emission_matrix.data[0, 1] = np.log(0.4)\n",
    "model.emission_model.unnormalized_emission_matrix.data[0, 2] = np.log(0.5)\n",
    "model.emission_model.unnormalized_emission_matrix.data[1, 0] = np.log(0.7)\n",
    "model.emission_model.unnormalized_emission_matrix.data[1, 1] = np.log(0.2)\n",
    "model.emission_model.unnormalized_emission_matrix.data[1, 2] = np.log(0.1)\n",
    "\n",
    "#Transposed !!!\n",
    "model.transition_model.unnormalized_transition_matrix.data[0,0] = np.log(0.7)\n",
    "model.transition_model.unnormalized_transition_matrix.data[0,1] = np.log(0.4)\n",
    "model.transition_model.unnormalized_transition_matrix.data[1,0] = np.log(0.3)\n",
    "model.transition_model.unnormalized_transition_matrix.data[1,1] = np.log(0.6)\n",
    "\n",
    "# In state 0, only allow consonants; in state 1, only allow vowels\n",
    "#vowel_indices = torch.tensor([alphabet.index(letter) for letter in \"aeiou\"])\n",
    "#consonant_indices = torch.tensor([alphabet.index(letter) for letter in \"bcdfghjklmnpqrstvwxyz\"])\n",
    "#model.emission_model.unnormalized_emission_matrix[0, vowel_indices] = -np.inf\n",
    "#model.emission_model.unnormalized_emission_matrix[1, consonant_indices] = -np.inf \n",
    "print(\"Emission matrix:\", torch.nn.functional.softmax(model.emission_model.unnormalized_emission_matrix, dim=1))\n",
    "\n",
    "# Only allow vowel -> consonant and consonant -> vowel\n",
    "#model.transition_model.unnormalized_transition_matrix[0,0] = -np.inf  # consonant -> consonant\n",
    "#model.transition_model.unnormalized_transition_matrix[0,1] = 0.       # vowel -> consonant\n",
    "#model.transition_model.unnormalized_transition_matrix[1,0] = 0.       # consonant -> vowel\n",
    "#model.transition_model.unnormalized_transition_matrix[1,1] = -np.inf  # vowel -> vowel\n",
    "print(\"Transition matrix:\", torch.nn.functional.softmax(model.transition_model.unnormalized_transition_matrix, dim=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def emission_model_forward(self, x_t):\n",
    "  log_emission_matrix = torch.nn.functional.log_softmax(self.unnormalized_emission_matrix, dim=1)\n",
    "  out = log_emission_matrix[:, x_t].transpose(0,1)\n",
    "  return out\n",
    "\n",
    "def transition_model_forward(self, log_alpha):\n",
    "  \"\"\"\n",
    "  log_alpha : Tensor of shape (batch size, N)\n",
    "  Multiply previous timestep's alphas by transition matrix (in log domain)\n",
    "  \"\"\"\n",
    "  log_transition_matrix = torch.nn.functional.log_softmax(self.unnormalized_transition_matrix, dim=0)\n",
    "\n",
    "  # Matrix multiplication in the log domain\n",
    "  out = log_domain_matmul(log_transition_matrix, log_alpha.transpose(0,1)).transpose(0,1)\n",
    "  return out\n",
    "\n",
    "def log_domain_matmul(log_A, log_B):\n",
    "\t\"\"\"\n",
    "\tlog_A : m x n\n",
    "\tlog_B : n x p\n",
    "\toutput : m x p matrix\n",
    "\n",
    "\tNormally, a matrix multiplication\n",
    "\tcomputes out_{i,j} = sum_k A_{i,k} x B_{k,j}\n",
    "\n",
    "\tA log domain matrix multiplication\n",
    "\tcomputes out_{i,j} = logsumexp_k log_A_{i,k} + log_B_{k,j}\n",
    "\t\"\"\"\n",
    "\tm = log_A.shape[0]\n",
    "\tn = log_A.shape[1]\n",
    "\tp = log_B.shape[1]\n",
    "\n",
    "\tlog_A_expanded = torch.stack([log_A] * p, dim=2)\n",
    "\tlog_B_expanded = torch.stack([log_B] * m, dim=0)\n",
    "\n",
    "\telementwise_sum = log_A_expanded + log_B_expanded\n",
    "\tout = torch.logsumexp(elementwise_sum, dim=1)\n",
    "\n",
    "\treturn out\n",
    "\n",
    "TransitionModel.forward = transition_model_forward\n",
    "EmissionModel.forward = emission_model_forward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part a ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(self, x, T):\n",
    "  \"\"\"\n",
    "  x : IntTensor of shape (batch size, T_max)\n",
    "  T : IntTensor of shape (batch size)\n",
    "  Find argmax_z log p(x|z) for each (x) in the batch.\n",
    "  \"\"\"\n",
    "  if self.is_cuda:\n",
    "    x = x.cuda()\n",
    "    T = T.cuda()\n",
    "\n",
    "  batch_size = x.shape[0]; T_max = x.shape[1]\n",
    "  log_state_priors = torch.nn.functional.log_softmax(self.unnormalized_state_priors, dim=0)\n",
    "  log_delta = torch.zeros(batch_size, T_max, self.N).float()\n",
    "  psi = torch.zeros(batch_size, T_max, self.N).long()\n",
    "  if self.is_cuda:\n",
    "    log_delta = log_delta.cuda()\n",
    "    psi = psi.cuda()\n",
    "\n",
    "  log_delta[:, 0, :] = self.emission_model(x[:,0]) + log_state_priors\n",
    "  for t in range(1, T_max):\n",
    "    max_val, argmax_val = self.transition_model.maxmul(log_delta[:, t-1, :])\n",
    "    log_delta[:, t, :] = self.emission_model(x[:,t]) + max_val\n",
    "    psi[:, t, :] = argmax_val\n",
    "\n",
    "  # Get the log probability of the best path\n",
    "  log_max = log_delta.max(dim=2)[0]\n",
    "  best_path_scores = torch.gather(log_max, 1, T.view(-1,1) - 1)\n",
    "\n",
    "  # This next part is a bit tricky to parallelize across the batch,\n",
    "  # so we will do it separately for each example.\n",
    "  z_star = []\n",
    "  for i in range(0, batch_size):\n",
    "    z_star_i = [ log_delta[i, T[i] - 1, :].max(dim=0)[1].item() ]\n",
    "    for t in range(T[i] - 1, 0, -1):\n",
    "      z_t = psi[i, t, z_star_i[0]].item()\n",
    "      z_star_i.insert(0, z_t)\n",
    "\n",
    "    z_star.append(z_star_i)\n",
    "\n",
    "  return z_star, best_path_scores # return both the best path and its log probability\n",
    "\n",
    "def transition_model_maxmul(self, log_alpha):\n",
    "  log_transition_matrix = torch.nn.functional.log_softmax(self.unnormalized_transition_matrix, dim=0)\n",
    "\n",
    "  out1, out2 = maxmul(log_transition_matrix, log_alpha.transpose(0,1))\n",
    "  return out1.transpose(0,1), out2.transpose(0,1)\n",
    "\n",
    "def maxmul(log_A, log_B):\n",
    "\t\"\"\"\n",
    "\tlog_A : m x n\n",
    "\tlog_B : n x p\n",
    "\toutput : m x p matrix\n",
    "\n",
    "\tSimilar to the log domain matrix multiplication,\n",
    "\tthis computes out_{i,j} = max_k log_A_{i,k} + log_B_{k,j}\n",
    "\t\"\"\"\n",
    "\tm = log_A.shape[0]\n",
    "\tn = log_A.shape[1]\n",
    "\tp = log_B.shape[1]\n",
    "\n",
    "\tlog_A_expanded = torch.stack([log_A] * p, dim=2)\n",
    "\tlog_B_expanded = torch.stack([log_B] * m, dim=0)\n",
    "\n",
    "\telementwise_sum = log_A_expanded + log_B_expanded\n",
    "\tout1,out2 = torch.max(elementwise_sum, dim=1)\n",
    "\n",
    "\treturn out1,out2\n",
    "\n",
    "TransitionModel.maxmul = transition_model_maxmul\n",
    "HMM.viterbi = viterbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part b ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.stack( [torch.tensor(encode(\"cab\"))] )\n",
    "T = torch.tensor([3])\n",
    "temp = model.viterbi(x, T)\n",
    "print(temp[0])\n",
    "print(np.exp(temp[1].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
